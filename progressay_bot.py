# -*- coding: utf-8 -*-
"""Progressay Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibeAUFETvOtGVUntCiOftGOruAGBWMCl
"""



# !pip install langchain
# !pip install openai
# !pip install PyPDF2
# !pip install faiss-cpu
# !pip install tiktoken
# !pip install langchain_openai
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
import os
# !pip install langchain_openai
from langchain_openai import OpenAIEmbeddings
import os
from langchain_community.vectorstores import FAISS
os.environ["OPENAI_API_KEY"]="sk-proj-XT3rUshZlwHjJxMJR6AIT3BlbkFJgHx22jI7wMdQDUzSnj4X"
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
# Create a FAISS instance for vector database from 'data'
from langchain.document_loaders.csv_loader import CSVLoader
encoding = 'utf-8'
loader = CSVLoader(file_path='/content/Progressay1.csv', encoding=encoding)

# Store the loaded data in the 'data' variable
data = loader.load()
vectordb = FAISS.from_documents(documents=data,
                                 embedding=embeddings)

# Create a retriever for querying the vector database
retriever = vectordb.as_retriever(score_threshold = 0.7)

prompt_template = """Your name is Kira,a virtual teacher assistant.You have to proper guide to students about their roadmap and also answer about the question if it is asked in documents.Given the following context and a question, generate an answer based on this context only.
In the answer try to provide as much text as possible from "response" section in the source document context without making much changes.
If the answer is not found in the context, kindly state "I don't know." Don't try to make up an answer.
You've to generate the clean text because this text has to be present to user. Not start with \n or other special chararcter. Please be meticulous about this
CONTEXT: {context}

QUESTION: {question}"""


PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
chain_type_kwargs = {"prompt": PROMPT}


from langchain.chains import RetrievalQA
llm = OpenAI(temperature=0.5)

chain = RetrievalQA.from_chain_type(llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            input_key="query",
                            chain_type_kwargs=chain_type_kwargs)
chain('What is Grading assistant')['result'].split("\n")[-1]